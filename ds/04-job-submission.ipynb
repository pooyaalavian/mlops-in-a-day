{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Submit Jobs\n",
        "## AML Cluster"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to workspace\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"49b5441f-dda4-47a9-81c4-13272430f4ff\",\n",
        "    resource_group_name=\"rg-pooya120-dev\",\n",
        "    workspace_name=\"amlwpooya120dev\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1682515447104
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Resource\n",
        "We search for an existing cluster. If it does not exist, we create a new one.\n",
        "\n",
        "Check your Compute page in Azure ML to confirm the process is successful."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a compute resource\n",
        "\n",
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\")\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        type=\"amlcompute\",\n",
        "        size=\"STANDARD_DS3_V2\",\n",
        "        min_instances=0,\n",
        "        max_instances=4,\n",
        "        idle_time_before_scale_down=180,\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "    print(f\"{cpu_cluster.name} will be created... compute size {cpu_cluster.size}...\", end='')\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)\n",
        "    print(\"Done!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named cpu-cluster, we'll reuse it as is.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1682515534423
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Job Environment\n",
        "We create an environment that has the packages we need installed. We register it for future use.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Create an empty folder for the config file.\n",
        "2. Create a `conda.yml` file containing the requirements of your environment.\n",
        "3. Create an environment according to the `conda.yml` file and register it to the workspace.\n",
        "\n",
        "Check your Environments page in Azure ML to confirm the process is successful."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dependencies_dir = \"./04-job-submission/dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1682515571261
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {dependencies_dir}/conda.yml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.21.2\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=0.24.2\n",
        "  - scipy=1.7.1\n",
        "  - pandas>=1.1,<1.2\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - xlrd==2.0.1\n",
        "    - mlflow== 1.26.1\n",
        "    - azureml-mlflow==1.42.0\n",
        "    - psutil>=5.8,<5.9\n",
        "    - tqdm>=4.59,<4.60\n",
        "    - ipykernel~=6.0\n",
        "    - matplotlib"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./04-job-submission/dependencies/conda.yml\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"bike-share-train-env\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Bike Share pipeline\",\n",
        "    tags={\"scikit-learn\": \"0.24.2\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment {pipeline_job_env.name} registered to workspace. \"\n",
        "    f\"Environment version is {pipeline_job_env.version}.\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment bike-share-train-env registered to workspace. Environment version is 1.\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1682515599110
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Command Job\n",
        "We create an Azure ML command job to train a model for credit default prediction. The command job is used to run a training script in the created environment on the created compute resource. \n",
        "\n",
        "The training script handles the data preparation, training and registering of the trained model. In this tutorial, you'll create a Python training script.\n",
        "\n",
        "Command jobs can be run from CLI, Python SDK, or studio interface. In this tutorial, you'll use the Azure Machine Learning Python SDK v2 to create and run the command job.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Create an empty folder to contain the code. (This is a good practice, both from organization aspect and performance.)\n",
        "2. Write your scripts in this folder."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_src_dir = \"./04-job-submission/src\"\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1682515612276
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/parser.py\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
        "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
        "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./04-job-submission/src/parser.py\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/main.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from parser import parse_args\n",
        "\n",
        "\n",
        "def data_prep(data, test_train_ratio):\n",
        "    print(\"input data:\", data)\n",
        "    \n",
        "    df = pd.read_csv(data, index_col=0)\n",
        "    df['hr_sin'] =  df['hr'].apply(lambda x: np.sin(2 * np.pi * x / 24))\n",
        "    df['hr_cos'] =  df['hr'].apply(lambda x: np.cos(2 * np.pi * x / 24))\n",
        "    df['temp2'] = (df['temp'] - 0.6)**2\n",
        "    X = df[['hr', 'hr_sin', 'hr_cos', 'temp', 'temp2', 'hum', 'windspeed','workingday', 'weathersit' ]]\n",
        "    y = df['cnt']\n",
        "    mlflow.log_metric(\"num_samples\", X.shape[0])\n",
        "    mlflow.log_metric(\"num_features\", X.shape[1] )\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_train_ratio,)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def train_model(X_train, X_test, y_train, y_test, n_estimators, learning_rate):\n",
        "\n",
        "    # convert the dataframe values to array\n",
        "    X_train = X_train.values\n",
        "    X_test = X_test.values\n",
        "\n",
        "    print(f\"Training with data of shape {X_train.shape}\")\n",
        "\n",
        "    # clf = GradientBoostingRegressor( n_estimators=n_estimators, learning_rate=learning_rate)\n",
        "    clf = RandomForestRegressor( n_estimators=n_estimators, )\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mlflow.log_metric('rmse',rmse)\n",
        "    \n",
        "    mlflow.log_metric('r2_score',r2_score(y_test, y_pred))\n",
        "    return clf\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    args = parse_args()\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    # enable autologging\n",
        "    mlflow.sklearn.autolog()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = data_prep(args.data, args.test_train_ratio)\n",
        "\n",
        "    model = train_model(X_train, X_test, y_train, y_test, args.n_estimators, args.learning_rate)\n",
        "\n",
        "    # Registering the model to the workspace\n",
        "    print(\"Registering the model via MLFlow\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=model,\n",
        "        registered_model_name=args.registered_model_name,\n",
        "        artifact_path=args.registered_model_name,\n",
        "    )\n",
        "\n",
        "    # Saving the model to a file\n",
        "    mlflow.sklearn.save_model(\n",
        "        sk_model=model,\n",
        "        path=os.path.join(args.registered_model_name, \"trained_model\"),\n",
        "    )\n",
        "    \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./04-job-submission/src/main.py\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure and Submit\n",
        "Create a command job from the files and submit it for execution.\n",
        "\n",
        "After the job is submitted, you can check the status by going to Jobs page of Azure ML and finding the experiment named `experiment_name`. There, click on the most recent run."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input\n",
        "\n",
        "registered_model_name = \"job_model\"\n",
        "experiment_name=\"bike-share-exp\"\n",
        "\n",
        "data = Input(type=\"uri_file\", path=\"azureml:hourly:1\",)\n",
        "\n",
        "job = command(\n",
        "    inputs={\n",
        "        \"data\": data,\n",
        "        \"test_train_ratio\": 0.2,\n",
        "        \"learning_rate\": 0.25,\n",
        "        \"registered_model_name\": registered_model_name,\n",
        "    },\n",
        "    code= train_src_dir,  # location of source code\n",
        "    command=\"python main.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --learning_rate ${{inputs.learning_rate}} --registered_model_name ${{inputs.registered_model_name}}\",\n",
        "    environment=f\"{custom_env_name}@latest\",\n",
        "    compute=cpu_compute_target,\n",
        "    experiment_name=experiment_name,\n",
        "    display_name=\"cluster-job\",\n",
        ")\n",
        "\n",
        "# Submit the job\n",
        "ml_client.create_or_update(job)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading src (0.0 MBs):   0%|          | 0/3461 [00:00<?, ?it/s]\r\u001b[32mUploading src (0.0 MBs): 100%|██████████| 3461/3461 [00:00<00:00, 57800.75it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "Command({'parameters': {}, 'init': False, 'type': 'command', 'status': 'Starting', 'log_files': None, 'name': 'witty_bone_flfpxgt7nk', 'description': None, 'tags': {}, 'properties': {'_azureml.ComputeTargetType': 'amlctrain', 'ContentSnapshotId': '7ca523d4-19e3-4b3d-ba63-3622329baa7d'}, 'print_as_yaml': True, 'id': '/subscriptions/49b5441f-dda4-47a9-81c4-13272430f4ff/resourceGroups/rg-pooya120-dev/providers/Microsoft.MachineLearningServices/workspaces/amlwpooya120dev/jobs/witty_bone_flfpxgt7nk', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/pooyaalavian1/code/Users/pooyaalavian/experimentation/ds', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f68bb4104f0>, 'serialize': <msrest.serialization.Serializer object at 0x7f68bb410640>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'cluster-job', 'experiment_name': 'bike-share-exp', 'compute': 'cpu-cluster', 'services': {'Tracking': <azure.ai.ml.entities._job.job_service.JobService object at 0x7f68e05bb1c0>, 'Studio': <azure.ai.ml.entities._job.job_service.JobService object at 0x7f68e05bb280>}, 'comment': None, 'job_inputs': {'data': {'type': 'uri_file', 'path': 'hourly:1', 'mode': 'ro_mount'}, 'test_train_ratio': '0.2', 'learning_rate': '0.25', 'registered_model_name': 'job_model'}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.witty_bone_flfpxgt7nk', 'mode': 'rw_mount'}}, 'inputs': {'data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f68bb411ab0>, 'test_train_ratio': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f68bb411720>, 'learning_rate': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f68bb413010>, 'registered_model_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f68bb411840>}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f68bb411900>}, 'component': CommandComponent({'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'name': 'witty_bone_flfpxgt7nk', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('.'), 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f68bb4104f0>, 'serialize': <msrest.serialization.Serializer object at 0x7f68bb413070>, 'command': 'python main.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --learning_rate ${{inputs.learning_rate}} --registered_model_name ${{inputs.registered_model_name}}', 'code': '/subscriptions/49b5441f-dda4-47a9-81c4-13272430f4ff/resourceGroups/rg-pooya120-dev/providers/Microsoft.MachineLearningServices/workspaces/amlwpooya120dev/codes/5da93d80-fcc1-457b-909e-2189fd3c1738/versions/1', 'environment_variables': {}, 'environment': '/subscriptions/49b5441f-dda4-47a9-81c4-13272430f4ff/resourceGroups/rg-pooya120-dev/providers/Microsoft.MachineLearningServices/workspaces/amlwpooya120dev/environments/bike-share-train-env/versions/1', 'distribution': None, 'resources': None, 'queue_settings': None, 'version': None, 'latest_version': None, 'schema': None, 'type': 'command', 'display_name': 'cluster-job', 'is_deterministic': True, 'inputs': {'data': {'type': 'uri_file', 'path': '/subscriptions/49b5441f-dda4-47a9-81c4-13272430f4ff/resourceGroups/rg-pooya120-dev/providers/Microsoft.MachineLearningServices/workspaces/amlwpooya120dev/data/hourly/versions/1', 'mode': 'ro_mount'}, 'test_train_ratio': {'type': 'string', 'default': '0.2'}, 'learning_rate': {'type': 'string', 'default': '0.25'}, 'registered_model_name': {'type': 'string', 'default': 'job_model'}}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.witty_bone_flfpxgt7nk', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': <azure.ai.ml.entities._job.job_service.JobService object at 0x7f68e05bb1c0>, 'Studio': <azure.ai.ml.entities._job.job_service.JobService object at 0x7f68e05bb280>}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f68bb4104f0>}, 'instance_id': 'b2b1ee8c-ed1d-4ce6-b651-9d29af20ea70', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': 'bike-share-train-env:1', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'queue_settings': None, 'swept': False})",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>bike-share-exp</td><td>witty_bone_flfpxgt7nk</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/witty_bone_flfpxgt7nk?wsid=/subscriptions/49b5441f-dda4-47a9-81c4-13272430f4ff/resourcegroups/rg-pooya120-dev/workspaces/amlwpooya120dev&amp;tid=16b3c013-d300-468d-ac64-7eda0820b6d3\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1682517185202
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recommendations\n",
        "\n",
        "As you become more familiar with Azure ML and your code base, it is a good practice to get away from Notebook and start writing python code. This is the best practice no matter you use Azure ML or not. \n",
        "\n",
        "We recommend using an IDE such as Visual Studio Code for such task. It provides a better experience for developing code. \n",
        "\n",
        "Real projects can have multiple files with dependencies. And handling them in notebook is not recommended.\n",
        "\n",
        "Using Azure ML SDK and CLI allows you to directly deploy code from your local machine (no need to use compute instance), if that is desired. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps\n",
        "If you are interested in deploying this model as an endpoint, [click here](https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-azure-ml-in-a-day)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}