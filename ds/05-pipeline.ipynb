{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Submit Pipelines\n",
        "## AML Cluster"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"49b5441f-dda4-47a9-81c4-13272430f4ff\",\n",
        "    resource_group_name=\"rg-pooya120-dev\",\n",
        "    workspace_name=\"amlwpooya120dev\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1682518539068
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Resource\n",
        "We search for an existing cluster. If it does not exist, we create a new one.\n",
        "\n",
        "Check your Compute page in Azure ML to confirm the process is successful."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a compute resource\n",
        "\n",
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\")\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        type=\"amlcompute\",\n",
        "        size=\"STANDARD_DS3_V2\",\n",
        "        min_instances=0,\n",
        "        max_instances=4,\n",
        "        idle_time_before_scale_down=180,\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "    print(f\"{cpu_cluster.name} will be created... compute size {cpu_cluster.size}...\", end='')\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)\n",
        "    print(\"Done!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named cpu-cluster, we'll reuse it as is.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1682518542203
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Job Environment\n",
        "We create an environment that has the packages we need installed. We register it for future use.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Create an empty folder for the config file.\n",
        "2. Create a `conda.yml` file containing the requirements of your environment.\n",
        "3. Create an environment according to the `conda.yml` file and register it to the workspace.\n",
        "\n",
        "Check your Environments page in Azure ML to confirm the process is successful."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dependencies_dir = \"./05-pipeline/dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1682518545289
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {dependencies_dir}/conda.yml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.21.2\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=0.24.2\n",
        "  - scipy=1.7.1\n",
        "  - pandas>=1.1,<1.2\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - xlrd==2.0.1\n",
        "    - mlflow== 1.26.1\n",
        "    - azureml-mlflow==1.42.0\n",
        "    - psutil>=5.8,<5.9\n",
        "    - tqdm>=4.59,<4.60\n",
        "    - ipykernel~=6.0\n",
        "    - matplotlib"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./05-pipeline/dependencies/conda.yml\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"bike-share-train\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Bike Share pipeline\",\n",
        "    tags={\"scikit-learn\": \"0.24.2\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment {pipeline_job_env.name} registered to workspace. \"\n",
        "    f\"Environment version is {pipeline_job_env.version}.\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment bike-share-train registered to workspace. Environment version is 2.\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1682518557321
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Registration\n",
        "It is easier to pass registered data assets to pipeline steps, rather than pure imports (as shown in Notebook1).\n",
        "\n",
        "Here, we create the same dataset, and register it to Azure ML.\n",
        "\n",
        "Note that the file is not downloaded to Azure ML. It still resides at the source. Azure ML provides a uniform file path to access this data, as you will see later. \n",
        "\n",
        "Confirm the data asset is registered by visiting the \"Data\" page of your Azure ML workspace."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1682518573134
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline Components\n",
        "We create the following pipeline components:\n",
        "\n",
        "1. Data preparation\n",
        "2. Model training\n",
        "\n",
        "Each component consists of one or more python files. The files for each component are hosted in separate folders.\n",
        "\n",
        "For the sake of this demo, we create component 1 using pure python, and component 2 using python + `yml` file.\n",
        "You can use whichever you like. Also, there is a third way (`yml`+ az cli), which we'll not discuss here.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_prep_src_dir = \"./05-pipeline/components/01_data_prep\"\n",
        "os.makedirs(data_prep_src_dir, exist_ok=True)\n",
        "\n",
        "train_src_dir = \"./05-pipeline/components/02_train\"\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1682518577135
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writing data_prep file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {data_prep_src_dir}/data_prep.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "import mlflow\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.data)\n",
        "\n",
        "    df = pd.read_csv(args.data, index_col=0)\n",
        "\n",
        "\n",
        "      \n",
        "    df['hr_sin'] =  df['hr'].apply(lambda x: np.sin(2 * np.pi * x / 24))\n",
        "    df['hr_cos'] =  df['hr'].apply(lambda x: np.cos(2 * np.pi * x / 24))\n",
        "    df['temp2'] = (df['temp'] - 0.6)**2\n",
        "    # X = df[['hr', 'hr_sin', 'hr_cos', 'temp', 'temp2', 'hum', 'windspeed','workingday', 'weathersit' ]]\n",
        "    # y = df['cnt']\n",
        "\n",
        "    df = df[['hr', 'hr_sin', 'hr_cos', 'temp', 'temp2', 'hum', 'windspeed','workingday', 'weathersit', 'hr_sin','hr_cos','temp2','cnt']]\n",
        "\n",
        "    mlflow.log_metric(\"num_samples\", df.shape[0])\n",
        "    mlflow.log_metric(\"num_features\", df.shape[1] - 1)\n",
        "\n",
        "\n",
        "    train_df, test_df = train_test_split(\n",
        "        df,\n",
        "        test_size=args.test_train_ratio,\n",
        "    )\n",
        "\n",
        "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
        "    train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
        "\n",
        "    test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./05-pipeline/components/01_data_prep/data_prep.py\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writing train file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.py\n",
        "import argparse\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import classification_report, mean_squared_error, r2_score\n",
        "import os\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import numpy as np \n",
        "\n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "# Start Logging\n",
        "mlflow.start_run()\n",
        "\n",
        "# enable autologging\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
        "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
        "    train_df = pd.read_csv(select_first_file(args.train_data))\n",
        "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
        "\n",
        "    y_train = train_df.pop('cnt')\n",
        "    y_test = test_df.pop('cnt')\n",
        "    X_train = train_df.values\n",
        "    X_test = test_df.values\n",
        "\n",
        "    print(f\"Training with data of shape {X_train.shape}\")\n",
        "\n",
        "    # clf = GradientBoostingRegressor( n_estimators=n_estimators, learning_rate=learning_rate)\n",
        "    clf = RandomForestRegressor( n_estimators=args.n_estimators, )\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mlflow.log_metric('rmse',rmse)\n",
        "    \n",
        "    mlflow.log_metric('r2_score',r2_score(y_test, y_pred))\n",
        "\n",
        "    # Registering the model to the workspace\n",
        "    print(\"Registering the model via MLFlow\")\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=clf,\n",
        "        registered_model_name=args.registered_model_name,\n",
        "        artifact_path=args.registered_model_name,\n",
        "    )\n",
        "\n",
        "    # Saving the model to a file\n",
        "    mlflow.sklearn.save_model(\n",
        "        sk_model=clf,\n",
        "        path=os.path.join(args.model, \"trained_model\"),\n",
        "    )\n",
        "\n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./05-pipeline/components/02_train/train.py\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writing train component `yml` file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {train_src_dir}/train.yml\n",
        "# <component>\n",
        "name: train_bike_share_model\n",
        "display_name: 'Bike Share Pipeline - Train Model'\n",
        "# version: 1 # Not specifying a version will automatically update the version\n",
        "type: command\n",
        "inputs:\n",
        "  train_data: \n",
        "    type: uri_folder\n",
        "  test_data: \n",
        "    type: uri_folder\n",
        "  learning_rate:\n",
        "    type: number\n",
        "  n_estimators:\n",
        "    type: number   \n",
        "  registered_model_name:\n",
        "    type: string\n",
        "outputs:\n",
        "  model:\n",
        "    type: uri_folder\n",
        "code: .\n",
        "environment:\n",
        "  # for this step, we'll use an AzureML curated environment\n",
        "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
        "command: >-\n",
        "  python train.py \n",
        "  --train_data ${{inputs.train_data}} \n",
        "  --test_data ${{inputs.test_data}} \n",
        "  --learning_rate ${{inputs.learning_rate}}\n",
        "  --registered_model_name ${{inputs.registered_model_name}} \n",
        "  --model ${{outputs.model}}\n",
        "version: 1.0.4\n",
        "# </component>"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./05-pipeline/components/02_train/train.yml\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating and Registering Components"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "DataPrepComponent = command(\n",
        "    name=\"bike_share_data_prep\",\n",
        "    display_name=\"Bike Share Pipeline - Data preparation\",\n",
        "    description=\"reads a csv input, split the input to train and test\",\n",
        "    inputs={\n",
        "        \"data\": Input(type=\"uri_folder\"),\n",
        "        \"test_train_ratio\": Input(type=\"number\", default=0.3),\n",
        "    },\n",
        "    outputs={\n",
        "        \"train_data\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        \"test_data\": Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    },\n",
        "    # The source folder of the component\n",
        "    code=data_prep_src_dir,\n",
        "    command=\"\"\"python data_prep.py \\\n",
        "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
        "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
        "            \"\"\",\n",
        "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
        "    version=\"2.0.6\",\n",
        ").component # `command` is a component builder, to get the actual component, you must use command(...).component\n",
        "\n",
        "\n",
        "# importing the Component Package\n",
        "from azure.ai.ml import load_component\n",
        "\n",
        "# Loading the component from the yml file\n",
        "TrainComponent = load_component(source=os.path.join(train_src_dir, \"train.yml\"))"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1682519525179
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we register the components to the workspace\n",
        "DataPrepComponent = ml_client.create_or_update(DataPrepComponent)\n",
        "TrainComponent = ml_client.create_or_update(TrainComponent)\n",
        "\n",
        "for c in [DataPrepComponent, TrainComponent]:\n",
        "    print(f\"Component {c.name} with Version {c.version} is registered\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading 01_data_prep (0.0 MBs):   0%|          | 0/2112 [00:00<?, ?it/s]\r\u001b[32mUploading 01_data_prep (0.0 MBs): 100%|██████████| 2112/2112 [00:00<00:00, 70334.11it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Component bike_share_data_prep with Version 2.0.6 is registered\nComponent train_bike_share_model with Version 1.0.4 is registered\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1682519532042
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipelines \n",
        "\n",
        "Now that components are ready, we can create a pipeline. \n",
        "\n",
        "A pipeline consists of steps (jobs). Each job is an instance of a component. \n",
        "\n",
        "For instance, you can create a component, `Component1`, which takes two numbers `a` and `b` and returns their sum. \n",
        "\n",
        "```py\n",
        "Component1 = command(\n",
        "    inputs={\n",
        "        \"a\": Input(type=\"number\"),\n",
        "        \"b\": Input(type=\"number\"),\n",
        "    },\n",
        "    outputs={\n",
        "        \"result\": Output(type=\"number\"),\n",
        "    },\n",
        "    command=\"python calc.py -a ${{inputs.a}} -b ${{inputs.b}}\", ...\n",
        ")\n",
        "```\n",
        "\n",
        "You can create jobs from this component, by calling the component and providing the inputs:\n",
        "```py\n",
        "job1 = Component1(a=5, b=6)\n",
        "job2 = Component1(a=7, b=8)\n",
        "```\n",
        "\n",
        "You can then create a pipeline that consists of these two jobs.\n",
        "\n",
        "```py\n",
        "@dsl.pipeline(compute=cpu_compute_target)\n",
        "def MyPipeline():\n",
        "    job1 = Component1(a=5, b=6)\n",
        "    job2 = Component1(a=7, b=8)\n",
        "    return {\n",
        "        \"job1\": job1.outputs.result,\n",
        "        \"job2\": job2.outputs.result,\n",
        "    }\n",
        "```\n",
        "\n",
        "In this pipeline, jobs `job1` and `job2` are independent and can be executed in parallel.\n",
        "\n",
        "But you can get more creative. Let's say, your pipeline consists of `job1` that adds `5` and `6`. And `job2` that adds result of `job1` with `9`.\n",
        "\n",
        "```py\n",
        "@dsl.pipeline(compute=cpu_compute_target)\n",
        "def MyPipeline():\n",
        "    job1 = Component1(a=5, b=6)\n",
        "    job2 = Component1(a=job1.outputs.result, b=8)\n",
        "    return {\n",
        "        \"job1\": job1.outputs.result,\n",
        "        \"job2\": job2.outputs.result,\n",
        "    }\n",
        "```\n",
        "\n",
        "If your initial values must be customized per pipeline, you can modify the above code to:\n",
        "```py\n",
        "@dsl.pipeline(compute=cpu_compute_target)\n",
        "def MyPipeline(n1, n2, n3):\n",
        "    job1 = Component1(a=n1, b=n2)\n",
        "    job2 = Component1(a=job1.outputs.result, b=n3)\n",
        "    return {\n",
        "        \"job1\": job1.outputs.result,\n",
        "        \"job2\": job2.outputs.result,\n",
        "    }\n",
        "```\n",
        "\n",
        "Finally to submit a pipeline for execution, create an instance of the pipeline, with your inputs:\n",
        "```py\n",
        "my_pipeline = MyPipeline(5,6,8)\n",
        "my_pipeline_job = ml_client.jobs.create_or_update(my_pipeline,...)\n",
        "```\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_target,\n",
        "    description=\"Pipeline: Bike Share\",\n",
        ")\n",
        "def BikeSharePipeline(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_learning_rate,\n",
        "    pipeline_job_n_estimators,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    data_prep_job = DataPrepComponent(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    train_job = TrainComponent(\n",
        "        train_data=data_prep_job.outputs.train_data, \n",
        "        test_data=data_prep_job.outputs.test_data, \n",
        "        learning_rate=pipeline_job_learning_rate, \n",
        "        n_estimators=pipeline_job_n_estimators,\n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
        "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1682519536425
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate and Submit\n",
        "Create a pipeline instance and submit it.\n",
        "\n",
        "After the job is submitted, you can check the status by going to the Experiments page of Azure ML, or to the printed link."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"pipeline_bike_share_model\"\n",
        "\n",
        "pipeline = BikeSharePipeline(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path='azureml:hourly:1'),\n",
        "    pipeline_job_test_train_ratio=0.25,\n",
        "    pipeline_job_learning_rate=0.05,\n",
        "    pipeline_job_n_estimators=80,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    experiment_name=\"bike-share-exp\",\n",
        ")\n",
        "print(pipeline_job.studio_url)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "https://ml.azure.com/runs/affable_oxygen_qn0rf6p1dv?wsid=/subscriptions/49b5441f-dda4-47a9-81c4-13272430f4ff/resourcegroups/rg-pooya120-dev/workspaces/amlwpooya120dev&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1682519541050
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline with Hyper-parameter Sweep"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "from azure.ai.ml.sweep import Uniform, Choice\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_target,\n",
        "    description=\"Bike Share Pipeline: Hyper Param Sweep\",\n",
        ")\n",
        "def CreditDefaultsPipelineWithSweep(\n",
        "    pipeline_job_data_input,\n",
        "    pipeline_job_test_train_ratio,\n",
        "    pipeline_job_registered_model_name,\n",
        "):\n",
        "    data_prep_job = DataPrepComponent(\n",
        "        data=pipeline_job_data_input,\n",
        "        test_train_ratio=pipeline_job_test_train_ratio,\n",
        "    )\n",
        "\n",
        "    train_job = TrainComponent(\n",
        "        train_data=data_prep_job.outputs.train_data, \n",
        "        test_data=data_prep_job.outputs.test_data, \n",
        "        learning_rate=0.1, \n",
        "        n_estimators=Choice([10,20,50,100,150]), \n",
        "        registered_model_name=pipeline_job_registered_model_name,\n",
        "    )\n",
        "\n",
        "    sweep_step = train_job.sweep(\n",
        "        primary_metric=\"training_f1_score\",\n",
        "        goal=\"minimize\",\n",
        "        sampling_algorithm=\"random\",\n",
        "        compute=\"cpu-cluster\",\n",
        "    )\n",
        "    sweep_step.set_limits(max_total_trials=5, max_concurrent_trials=5, timeout=7200)\n",
        "\n",
        "\n",
        "    # a pipeline returns a dictionary of outputs\n",
        "    # keys will code for the pipeline output identifier\n",
        "    return {\n",
        "        \"sweep_result\": sweep_step.outputs.model\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1682518809329
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"pipeline_credit_defaults_model_sweep\"\n",
        "\n",
        "pipeline = CreditDefaultsPipelineWithSweep(\n",
        "    pipeline_job_data_input=Input(type=\"uri_file\", path='azureml:hourly:1'),\n",
        "    pipeline_job_test_train_ratio=0.25,\n",
        "    pipeline_job_registered_model_name=registered_model_name,\n",
        ")\n",
        "\n",
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    experiment_name=\"bike-share-exp\",\n",
        ")\n",
        "print(pipeline_job.studio_url)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "https://ml.azure.com/runs/crimson_peach_77fj9t0837?wsid=/subscriptions/49b5441f-dda4-47a9-81c4-13272430f4ff/resourcegroups/rg-pooya120-dev/workspaces/amlwpooya120dev&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1682518945108
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps\n",
        "If you are interested in deploying this model as an endpoint, [click here](https://github.com/Azure/azureml-examples/blob/main/tutorials/e2e-ds-experience/e2e-ml-workflow.ipynb)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}